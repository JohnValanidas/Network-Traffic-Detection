{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78e4334f",
   "metadata": {},
   "source": [
    "General Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "311f460b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy imports\n",
    "import numpy as np\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, default_collate, TensorDataset\n",
    "\n",
    "# scipy\n",
    "from scipy.io import arff\n",
    "\n",
    "# pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "82654999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f41eec07bd0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup pytorch device\n",
    "# I have a GPU (RTX 3080) so I want to take advantage of that\n",
    "\n",
    "use_cuda = True # simple way to disable in case there are issues\n",
    "\n",
    "if torch.cuda.is_available() and use_cuda:\n",
    "    my_device = torch.device(\"cuda\")\n",
    "else:\n",
    "    my_device = torch.device(\"cpu\")\n",
    "print('Device: {}'.format(my_device))\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "821b0f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 16 16:42:26 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.85.02    Driver Version: 510.85.02    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:09:00.0  On |                  N/A |\n",
      "| 54%   59C    P2   118W / 400W |   3031MiB / 12288MiB |      1%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1570      G   /usr/lib/xorg/Xorg                656MiB |\n",
      "|    0   N/A  N/A      3137      G   ...veSuggestionsOnlyOnDemand      167MiB |\n",
      "|    0   N/A  N/A      4030      G   /usr/lib/firefox/firefox          184MiB |\n",
      "|    0   N/A  N/A      6104      G   ...RendererForSitePerProcess      248MiB |\n",
      "|    0   N/A  N/A     18896      G   ...921148776468143732,131072       38MiB |\n",
      "|    0   N/A  N/A     70264      G   ...AAAAAAAAA= --shared-files       38MiB |\n",
      "|    0   N/A  N/A     72042      G   kitty                               6MiB |\n",
      "|    0   N/A  N/A     83958      C   ...conda3/envs/ML/bin/python      653MiB |\n",
      "|    0   N/A  N/A     94450      G   ...750355680810264907,131072       10MiB |\n",
      "|    0   N/A  N/A    108321      C   ...conda3/envs/ML/bin/python     1021MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1854cdf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>protocol_type</th>\n",
       "      <th>service</th>\n",
       "      <th>flag</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>land</th>\n",
       "      <th>wrong_fragment</th>\n",
       "      <th>urgent</th>\n",
       "      <th>hot</th>\n",
       "      <th>...</th>\n",
       "      <th>dst_host_srv_count</th>\n",
       "      <th>dst_host_same_srv_rate</th>\n",
       "      <th>dst_host_diff_srv_rate</th>\n",
       "      <th>dst_host_same_src_port_rate</th>\n",
       "      <th>dst_host_srv_diff_host_rate</th>\n",
       "      <th>dst_host_serror_rate</th>\n",
       "      <th>dst_host_srv_serror_rate</th>\n",
       "      <th>dst_host_rerror_rate</th>\n",
       "      <th>dst_host_srv_rerror_rate</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>ftp_data</td>\n",
       "      <td>SF</td>\n",
       "      <td>491</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>25</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>udp</td>\n",
       "      <td>other</td>\n",
       "      <td>SF</td>\n",
       "      <td>146</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>private</td>\n",
       "      <td>S0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>anomaly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>232</td>\n",
       "      <td>8153</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>255</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>199</td>\n",
       "      <td>420</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>255</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   duration protocol_type   service flag  src_bytes  dst_bytes  land  \\\n",
       "0         0           tcp  ftp_data   SF        491          0     0   \n",
       "1         0           udp     other   SF        146          0     0   \n",
       "2         0           tcp   private   S0          0          0     0   \n",
       "3         0           tcp      http   SF        232       8153     0   \n",
       "4         0           tcp      http   SF        199        420     0   \n",
       "\n",
       "   wrong_fragment  urgent  hot  ...  dst_host_srv_count  \\\n",
       "0               0       0    0  ...                  25   \n",
       "1               0       0    0  ...                   1   \n",
       "2               0       0    0  ...                  26   \n",
       "3               0       0    0  ...                 255   \n",
       "4               0       0    0  ...                 255   \n",
       "\n",
       "   dst_host_same_srv_rate  dst_host_diff_srv_rate  \\\n",
       "0                    0.17                    0.03   \n",
       "1                    0.00                    0.60   \n",
       "2                    0.10                    0.05   \n",
       "3                    1.00                    0.00   \n",
       "4                    1.00                    0.00   \n",
       "\n",
       "   dst_host_same_src_port_rate  dst_host_srv_diff_host_rate  \\\n",
       "0                         0.17                         0.00   \n",
       "1                         0.88                         0.00   \n",
       "2                         0.00                         0.00   \n",
       "3                         0.03                         0.04   \n",
       "4                         0.00                         0.00   \n",
       "\n",
       "   dst_host_serror_rate  dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
       "0                  0.00                      0.00                  0.05   \n",
       "1                  0.00                      0.00                  0.00   \n",
       "2                  1.00                      1.00                  0.00   \n",
       "3                  0.03                      0.01                  0.00   \n",
       "4                  0.00                      0.00                  0.00   \n",
       "\n",
       "   dst_host_srv_rerror_rate    class  \n",
       "0                      0.00   normal  \n",
       "1                      0.00   normal  \n",
       "2                      0.00  anomaly  \n",
       "3                      0.01   normal  \n",
       "4                      0.00   normal  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data = pd.read_csv('datasets/NSL-KDD/KDDTrain+_with_header.txt', header=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "27bec7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings = ['protocol_type', 'service', 'flag', 'class']\n",
    "column_mappings = {}\n",
    "for to_map in mappings:\n",
    "    data.get(to_map).unique()\n",
    "    unique_vals = data.get(to_map).unique()\n",
    "    data[to_map].replace(to_replace=unique_vals,\n",
    "            value= list(range(len(unique_vals))),\n",
    "            inplace=True)\n",
    "    \n",
    "    # set column mappings\n",
    "    column_mappings[to_map] = {}\n",
    "    for intMaping, key in enumerate(unique_vals):\n",
    "        column_mappings[to_map][intMaping] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bad1745b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layer, output_dim, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.hidden_layer = hidden_layer\n",
    "        self.output_dim = output_dim\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_layer).to(device)\n",
    "        self.activation = nn.ReLU().to(device)\n",
    "        self.linear2 = nn.Linear(hidden_layer, output_dim).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        return x.to(self.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ca2c425f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = data['class']\n",
    "del data['class']\n",
    "train = TensorDataset(torch.Tensor(np.array(data)), torch.Tensor(np.array(target)).type(torch.LongTensor))\n",
    "train_loader = DataLoader(train, batch_size = 50, shuffle = True, collate_fn=lambda x: tuple(x_.to(my_device) for x_ in default_collate(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b441d9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr = 0.001\n",
    "clf = LinearNetwork(41, 50, 2, my_device)\n",
    "clf.to(my_device)\n",
    "\n",
    "clfLinearCriterion = nn.CrossEntropyLoss().to(my_device)\n",
    "clfLinearOptimizer = torch.optim.SGD(clf.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b5407a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier: Epoch:     0 loss: 83.70534\n",
      "classifier: Epoch:     5 loss: 0.23327\n",
      "classifier: Epoch:    10 loss: 0.22101\n",
      "classifier: Epoch:    15 loss: 0.21779\n",
      "classifier: Epoch:    20 loss: 0.21220\n",
      "classifier: Epoch:    25 loss: 0.23662\n",
      "classifier: Epoch:    30 loss: 0.22653\n",
      "classifier: Epoch:    35 loss: 0.22361\n",
      "classifier: Epoch:    40 loss: 0.22590\n",
      "classifier: Epoch:    45 loss: 0.25269\n",
      "classifier: Epoch:    50 loss: 0.23094\n",
      "classifier: Epoch:    55 loss: 0.22612\n",
      "classifier: Epoch:    60 loss: 0.22737\n",
      "classifier: Epoch:    65 loss: 0.25090\n",
      "classifier: Epoch:    70 loss: 0.29684\n",
      "classifier: Epoch:    75 loss: 0.29003\n",
      "classifier: Epoch:    80 loss: 0.20782\n",
      "classifier: Epoch:    85 loss: 0.29152\n",
      "classifier: Epoch:    90 loss: 0.29633\n",
      "classifier: Epoch:    95 loss: 0.29112\n",
      "classifier: Epoch:   100 loss: 0.28749\n",
      "classifier: Epoch:   105 loss: 0.28721\n",
      "classifier: Epoch:   110 loss: 0.28703\n",
      "classifier: Epoch:   115 loss: 0.28670\n",
      "classifier: Epoch:   120 loss: 0.21056\n",
      "classifier: Epoch:   125 loss: 0.29053\n",
      "classifier: Epoch:   130 loss: 0.27572\n",
      "classifier: Epoch:   135 loss: 0.27447\n",
      "classifier: Epoch:   140 loss: 0.28629\n",
      "classifier: Epoch:   145 loss: 0.28553\n",
      "classifier: Epoch:   150 loss: 0.28555\n",
      "classifier: Epoch:   155 loss: 0.28525\n",
      "classifier: Epoch:   160 loss: 0.20131\n",
      "classifier: Epoch:   165 loss: 0.28588\n",
      "classifier: Epoch:   170 loss: 0.26530\n",
      "classifier: Epoch:   175 loss: 0.21748\n",
      "classifier: Epoch:   180 loss: 0.21070\n",
      "classifier: Epoch:   185 loss: 0.20833\n",
      "classifier: Epoch:   190 loss: 0.20654\n",
      "classifier: Epoch:   195 loss: 0.20449\n",
      "classifier: Epoch:   200 loss: 0.20254\n",
      "classifier: Epoch:   205 loss: 0.20080\n",
      "classifier: Epoch:   210 loss: 0.19931\n",
      "classifier: Epoch:   215 loss: 0.19783\n",
      "classifier: Epoch:   220 loss: 0.19656\n",
      "classifier: Epoch:   225 loss: 0.19807\n",
      "classifier: Epoch:   230 loss: 0.19676\n",
      "classifier: Epoch:   235 loss: 0.19606\n",
      "classifier: Epoch:   240 loss: 0.19615\n",
      "classifier: Epoch:   245 loss: 0.19543\n",
      "classifier: Epoch:   250 loss: 0.19511\n",
      "classifier: Epoch:   255 loss: 0.19431\n",
      "classifier: Epoch:   260 loss: 0.19719\n",
      "classifier: Epoch:   265 loss: 0.19328\n",
      "classifier: Epoch:   270 loss: 0.18970\n",
      "classifier: Epoch:   275 loss: 0.18948\n",
      "classifier: Epoch:   280 loss: 0.19566\n",
      "classifier: Epoch:   285 loss: 0.18932\n",
      "classifier: Epoch:   290 loss: 0.18870\n",
      "classifier: Epoch:   295 loss: 0.19985\n",
      "classifier: Epoch:   300 loss: 0.19632\n",
      "classifier: Epoch:   305 loss: 0.19521\n",
      "classifier: Epoch:   310 loss: 0.19350\n",
      "classifier: Epoch:   315 loss: 0.18828\n",
      "classifier: Epoch:   320 loss: 0.18962\n",
      "classifier: Epoch:   325 loss: 0.18888\n",
      "classifier: Epoch:   330 loss: 0.18847\n",
      "classifier: Epoch:   335 loss: 0.18845\n",
      "classifier: Epoch:   340 loss: 0.18490\n",
      "classifier: Epoch:   345 loss: 0.18227\n",
      "classifier: Epoch:   350 loss: 0.18198\n",
      "classifier: Epoch:   355 loss: 0.18154\n",
      "classifier: Epoch:   360 loss: 0.18147\n",
      "classifier: Epoch:   365 loss: 0.18129\n",
      "classifier: Epoch:   370 loss: 0.18142\n",
      "classifier: Epoch:   375 loss: 0.13960\n",
      "classifier: Epoch:   380 loss: 0.15557\n",
      "classifier: Epoch:   385 loss: 0.19515\n",
      "classifier: Epoch:   390 loss: 0.19017\n",
      "classifier: Epoch:   395 loss: 0.18855\n",
      "classifier: Epoch:   400 loss: 0.18779\n",
      "classifier: Epoch:   405 loss: 0.18700\n",
      "classifier: Epoch:   410 loss: 0.18618\n",
      "classifier: Epoch:   415 loss: 0.18523\n",
      "classifier: Epoch:   420 loss: 0.18506\n",
      "classifier: Epoch:   425 loss: 0.18461\n",
      "classifier: Epoch:   430 loss: 0.18423\n",
      "classifier: Epoch:   435 loss: 0.18401\n",
      "classifier: Epoch:   440 loss: 0.18379\n",
      "classifier: Epoch:   445 loss: 0.18357\n",
      "classifier: Epoch:   450 loss: 0.18347\n",
      "classifier: Epoch:   455 loss: 0.18342\n",
      "classifier: Epoch:   460 loss: 0.18324\n",
      "classifier: Epoch:   465 loss: 0.18313\n",
      "classifier: Epoch:   470 loss: 0.18301\n",
      "classifier: Epoch:   475 loss: 0.18297\n",
      "classifier: Epoch:   480 loss: 0.18275\n",
      "classifier: Epoch:   485 loss: 0.18262\n",
      "classifier: Epoch:   490 loss: 0.18428\n",
      "classifier: Epoch:   495 loss: 0.18241\n",
      "classifier: Epoch:   500 loss: 0.18265\n",
      "classifier: Epoch:   505 loss: 0.18228\n",
      "classifier: Epoch:   510 loss: 0.18256\n",
      "classifier: Epoch:   515 loss: 0.18248\n",
      "classifier: Epoch:   520 loss: 0.18224\n",
      "classifier: Epoch:   525 loss: 0.18211\n",
      "classifier: Epoch:   530 loss: 0.18232\n",
      "classifier: Epoch:   535 loss: 0.18212\n",
      "classifier: Epoch:   540 loss: 0.18201\n",
      "classifier: Epoch:   545 loss: 0.18205\n",
      "classifier: Epoch:   550 loss: 0.18186\n",
      "classifier: Epoch:   555 loss: 0.18203\n",
      "classifier: Epoch:   560 loss: 0.18177\n",
      "classifier: Epoch:   565 loss: 0.18166\n",
      "classifier: Epoch:   570 loss: 0.18207\n",
      "classifier: Epoch:   575 loss: 0.18187\n",
      "classifier: Epoch:   580 loss: 0.18192\n",
      "classifier: Epoch:   585 loss: 0.18164\n",
      "classifier: Epoch:   590 loss: 0.18172\n",
      "classifier: Epoch:   595 loss: 0.18152\n",
      "classifier: Epoch:   600 loss: 0.18135\n",
      "classifier: Epoch:   605 loss: 0.18172\n",
      "classifier: Epoch:   610 loss: 0.18120\n",
      "classifier: Epoch:   615 loss: 0.18150\n",
      "classifier: Epoch:   620 loss: 0.18107\n",
      "classifier: Epoch:   625 loss: 0.18158\n",
      "classifier: Epoch:   630 loss: 0.18132\n",
      "classifier: Epoch:   635 loss: 0.18162\n",
      "classifier: Epoch:   640 loss: 0.18114\n",
      "classifier: Epoch:   645 loss: 0.18093\n",
      "classifier: Epoch:   650 loss: 0.18123\n",
      "classifier: Epoch:   655 loss: 0.18112\n",
      "classifier: Epoch:   660 loss: 0.18138\n",
      "classifier: Epoch:   665 loss: 0.18141\n",
      "classifier: Epoch:   670 loss: 0.18093\n",
      "classifier: Epoch:   675 loss: 0.18339\n",
      "classifier: Epoch:   680 loss: 0.18152\n",
      "classifier: Epoch:   685 loss: 0.18099\n",
      "classifier: Epoch:   690 loss: 0.18138\n",
      "classifier: Epoch:   695 loss: 0.18151\n",
      "classifier: Epoch:   700 loss: 0.12613\n",
      "classifier: Epoch:   705 loss: 0.12218\n",
      "classifier: Epoch:   710 loss: 0.18425\n",
      "classifier: Epoch:   715 loss: 0.18251\n",
      "classifier: Epoch:   720 loss: 0.18165\n",
      "classifier: Epoch:   725 loss: 0.18169\n",
      "classifier: Epoch:   730 loss: 0.18180\n",
      "classifier: Epoch:   735 loss: 0.18163\n",
      "classifier: Epoch:   740 loss: 0.18197\n",
      "classifier: Epoch:   745 loss: 0.18242\n",
      "classifier: Epoch:   750 loss: 0.18138\n",
      "classifier: Epoch:   755 loss: 0.18108\n",
      "classifier: Epoch:   760 loss: 0.18104\n",
      "classifier: Epoch:   765 loss: 0.18093\n",
      "classifier: Epoch:   770 loss: 0.18087\n",
      "classifier: Epoch:   775 loss: 0.18144\n",
      "classifier: Epoch:   780 loss: 0.18073\n",
      "classifier: Epoch:   785 loss: 0.18131\n",
      "classifier: Epoch:   790 loss: 0.18093\n",
      "classifier: Epoch:   795 loss: 0.18096\n",
      "classifier: Epoch:   800 loss: 0.18089\n",
      "classifier: Epoch:   805 loss: 0.18133\n",
      "classifier: Epoch:   810 loss: 0.18040\n",
      "classifier: Epoch:   815 loss: 0.18154\n",
      "classifier: Epoch:   820 loss: 0.18067\n",
      "classifier: Epoch:   825 loss: 0.18071\n",
      "classifier: Epoch:   830 loss: 0.18031\n",
      "classifier: Epoch:   835 loss: 0.18093\n",
      "classifier: Epoch:   840 loss: 0.18072\n",
      "classifier: Epoch:   845 loss: 0.18030\n",
      "classifier: Epoch:   850 loss: 0.18052\n",
      "classifier: Epoch:   855 loss: 0.18101\n",
      "classifier: Epoch:   860 loss: 0.18077\n",
      "classifier: Epoch:   865 loss: 0.18001\n",
      "classifier: Epoch:   870 loss: 0.18035\n",
      "classifier: Epoch:   875 loss: 0.18036\n",
      "classifier: Epoch:   880 loss: 0.18015\n",
      "classifier: Epoch:   885 loss: 0.18008\n",
      "classifier: Epoch:   890 loss: 0.18017\n",
      "classifier: Epoch:   895 loss: 0.18045\n",
      "classifier: Epoch:   900 loss: 0.18021\n",
      "classifier: Epoch:   905 loss: 0.18052\n",
      "classifier: Epoch:   910 loss: 0.18014\n",
      "classifier: Epoch:   915 loss: 0.18071\n",
      "classifier: Epoch:   920 loss: 0.18053\n",
      "classifier: Epoch:   925 loss: 0.18084\n",
      "classifier: Epoch:   930 loss: 0.18077\n",
      "classifier: Epoch:   935 loss: 0.18036\n",
      "classifier: Epoch:   940 loss: 0.18013\n",
      "classifier: Epoch:   945 loss: 0.18149\n",
      "classifier: Epoch:   950 loss: 0.18070\n",
      "classifier: Epoch:   955 loss: 0.18014\n",
      "classifier: Epoch:   960 loss: 0.18038\n",
      "classifier: Epoch:   965 loss: 0.17995\n",
      "classifier: Epoch:   970 loss: 0.17993\n",
      "classifier: Epoch:   975 loss: 0.17993\n",
      "classifier: Epoch:   980 loss: 0.17992\n",
      "classifier: Epoch:   985 loss: 0.18072\n",
      "classifier: Epoch:   990 loss: 0.18033\n",
      "classifier: Epoch:   995 loss: 0.17999\n",
      "classifier: Epoch:  1000 loss: 0.18017\n",
      "classifier: Epoch:  1005 loss: 0.17965\n",
      "classifier: Epoch:  1010 loss: 0.17981\n",
      "classifier: Epoch:  1015 loss: 0.17985\n",
      "classifier: Epoch:  1020 loss: 0.18022\n",
      "classifier: Epoch:  1025 loss: 0.17972\n",
      "classifier: Epoch:  1030 loss: 0.18001\n",
      "classifier: Epoch:  1035 loss: 0.17995\n",
      "classifier: Epoch:  1040 loss: 0.17990\n",
      "classifier: Epoch:  1045 loss: 0.18008\n",
      "classifier: Epoch:  1050 loss: 0.17961\n",
      "classifier: Epoch:  1055 loss: 0.18010\n",
      "classifier: Epoch:  1060 loss: 0.18002\n",
      "classifier: Epoch:  1065 loss: 0.17939\n",
      "classifier: Epoch:  1070 loss: 0.17974\n",
      "classifier: Epoch:  1075 loss: 0.17972\n",
      "classifier: Epoch:  1080 loss: 0.17994\n",
      "classifier: Epoch:  1085 loss: 0.17991\n",
      "classifier: Epoch:  1090 loss: 0.17981\n",
      "classifier: Epoch:  1095 loss: 0.17949\n",
      "classifier: Epoch:  1100 loss: 0.18003\n",
      "classifier: Epoch:  1105 loss: 0.17959\n",
      "classifier: Epoch:  1110 loss: 0.17962\n",
      "classifier: Epoch:  1115 loss: 0.17979\n",
      "classifier: Epoch:  1120 loss: 0.17963\n",
      "classifier: Epoch:  1125 loss: 0.17953\n",
      "classifier: Epoch:  1130 loss: 0.17924\n",
      "classifier: Epoch:  1135 loss: 0.17973\n",
      "classifier: Epoch:  1140 loss: 0.17985\n",
      "classifier: Epoch:  1145 loss: 0.17942\n",
      "classifier: Epoch:  1150 loss: 0.17973\n",
      "classifier: Epoch:  1155 loss: 0.17960\n",
      "classifier: Epoch:  1160 loss: 0.17932\n",
      "classifier: Epoch:  1165 loss: 0.17949\n",
      "classifier: Epoch:  1170 loss: 0.17950\n",
      "classifier: Epoch:  1175 loss: 0.17979\n",
      "classifier: Epoch:  1180 loss: 0.17962\n",
      "classifier: Epoch:  1185 loss: 0.17923\n",
      "classifier: Epoch:  1190 loss: 0.17953\n",
      "classifier: Epoch:  1195 loss: 0.17921\n",
      "classifier: Epoch:  1200 loss: 0.17993\n",
      "classifier: Epoch:  1205 loss: 0.17932\n",
      "classifier: Epoch:  1210 loss: 0.17933\n",
      "classifier: Epoch:  1215 loss: 0.17942\n",
      "classifier: Epoch:  1220 loss: 0.17944\n",
      "classifier: Epoch:  1225 loss: 0.17950\n",
      "classifier: Epoch:  1230 loss: 0.17916\n",
      "classifier: Epoch:  1235 loss: 0.17939\n",
      "classifier: Epoch:  1240 loss: 0.17938\n",
      "classifier: Epoch:  1245 loss: 0.17921\n",
      "classifier: Epoch:  1250 loss: 0.17950\n",
      "classifier: Epoch:  1255 loss: 0.17935\n",
      "classifier: Epoch:  1260 loss: 0.17909\n",
      "classifier: Epoch:  1265 loss: 0.17942\n",
      "classifier: Epoch:  1270 loss: 0.17916\n",
      "classifier: Epoch:  1275 loss: 0.17952\n",
      "classifier: Epoch:  1280 loss: 0.17934\n",
      "classifier: Epoch:  1285 loss: 0.17922\n",
      "classifier: Epoch:  1290 loss: 0.17939\n",
      "classifier: Epoch:  1295 loss: 0.17927\n",
      "classifier: Epoch:  1300 loss: 0.17924\n",
      "classifier: Epoch:  1305 loss: 0.17925\n",
      "classifier: Epoch:  1310 loss: 0.17943\n",
      "classifier: Epoch:  1315 loss: 0.17938\n",
      "classifier: Epoch:  1320 loss: 0.17917\n",
      "classifier: Epoch:  1325 loss: 0.17917\n",
      "classifier: Epoch:  1330 loss: 0.17921\n",
      "classifier: Epoch:  1335 loss: 0.17887\n",
      "classifier: Epoch:  1340 loss: 0.17968\n",
      "classifier: Epoch:  1345 loss: 0.17905\n",
      "classifier: Epoch:  1350 loss: 0.17935\n",
      "classifier: Epoch:  1355 loss: 0.17908\n",
      "classifier: Epoch:  1360 loss: 0.17908\n",
      "classifier: Epoch:  1365 loss: 0.17930\n",
      "classifier: Epoch:  1370 loss: 0.17896\n",
      "classifier: Epoch:  1375 loss: 0.17899\n",
      "classifier: Epoch:  1380 loss: 0.17897\n",
      "classifier: Epoch:  1385 loss: 0.17848\n",
      "classifier: Epoch:  1390 loss: 0.17933\n",
      "classifier: Epoch:  1395 loss: 0.17908\n",
      "classifier: Epoch:  1400 loss: 0.17908\n",
      "classifier: Epoch:  1405 loss: 0.17889\n",
      "classifier: Epoch:  1410 loss: 0.17939\n",
      "classifier: Epoch:  1415 loss: 0.17877\n",
      "classifier: Epoch:  1420 loss: 0.17909\n",
      "classifier: Epoch:  1425 loss: 0.17902\n",
      "classifier: Epoch:  1430 loss: 0.17899\n",
      "classifier: Epoch:  1435 loss: 0.17871\n",
      "classifier: Epoch:  1440 loss: 0.17897\n",
      "classifier: Epoch:  1445 loss: 0.17888\n",
      "classifier: Epoch:  1450 loss: 0.17887\n",
      "classifier: Epoch:  1455 loss: 0.17889\n",
      "classifier: Epoch:  1460 loss: 0.17885\n",
      "classifier: Epoch:  1465 loss: 0.17895\n",
      "classifier: Epoch:  1470 loss: 0.17921\n",
      "classifier: Epoch:  1475 loss: 0.17917\n",
      "classifier: Epoch:  1480 loss: 0.17894\n",
      "classifier: Epoch:  1485 loss: 0.17913\n",
      "classifier: Epoch:  1490 loss: 0.17905\n",
      "classifier: Epoch:  1495 loss: 0.17905\n",
      "classifier: Epoch:  1500 loss: 0.17888\n",
      "classifier: Epoch:  1505 loss: 0.17866\n",
      "classifier: Epoch:  1510 loss: 0.17873\n",
      "classifier: Epoch:  1515 loss: 0.17845\n",
      "classifier: Epoch:  1520 loss: 0.17894\n",
      "classifier: Epoch:  1525 loss: 0.17874\n",
      "classifier: Epoch:  1530 loss: 0.17891\n",
      "classifier: Epoch:  1535 loss: 0.17869\n",
      "classifier: Epoch:  1540 loss: 0.17875\n",
      "classifier: Epoch:  1545 loss: 0.17872\n",
      "classifier: Epoch:  1550 loss: 0.17879\n",
      "classifier: Epoch:  1555 loss: 0.17877\n",
      "classifier: Epoch:  1560 loss: 0.17876\n",
      "classifier: Epoch:  1565 loss: 0.17866\n",
      "classifier: Epoch:  1570 loss: 0.17878\n",
      "classifier: Epoch:  1575 loss: 0.17866\n",
      "classifier: Epoch:  1580 loss: 0.17880\n",
      "classifier: Epoch:  1585 loss: 0.17854\n",
      "classifier: Epoch:  1590 loss: 0.17879\n",
      "classifier: Epoch:  1595 loss: 0.17875\n",
      "classifier: Epoch:  1600 loss: 0.17880\n",
      "classifier: Epoch:  1605 loss: 0.17860\n",
      "classifier: Epoch:  1610 loss: 0.17889\n",
      "classifier: Epoch:  1615 loss: 0.17911\n",
      "classifier: Epoch:  1620 loss: 0.17879\n",
      "classifier: Epoch:  1625 loss: 0.17868\n",
      "classifier: Epoch:  1630 loss: 0.17867\n",
      "classifier: Epoch:  1635 loss: 0.17847\n",
      "classifier: Epoch:  1640 loss: 0.17835\n",
      "classifier: Epoch:  1645 loss: 0.17851\n",
      "classifier: Epoch:  1650 loss: 0.18462\n",
      "classifier: Epoch:  1655 loss: 0.18017\n",
      "classifier: Epoch:  1660 loss: 0.17954\n",
      "classifier: Epoch:  1665 loss: 0.17932\n",
      "classifier: Epoch:  1670 loss: 0.17899\n",
      "classifier: Epoch:  1675 loss: 0.17910\n",
      "classifier: Epoch:  1680 loss: 0.17909\n",
      "classifier: Epoch:  1685 loss: 0.17898\n",
      "classifier: Epoch:  1690 loss: 0.17897\n",
      "classifier: Epoch:  1695 loss: 0.17917\n",
      "classifier: Epoch:  1700 loss: 0.17916\n",
      "classifier: Epoch:  1705 loss: 0.17887\n",
      "classifier: Epoch:  1710 loss: 0.17917\n",
      "classifier: Epoch:  1715 loss: 0.17908\n",
      "classifier: Epoch:  1720 loss: 0.17921\n",
      "classifier: Epoch:  1725 loss: 0.17896\n",
      "classifier: Epoch:  1730 loss: 0.17900\n",
      "classifier: Epoch:  1735 loss: 0.17879\n",
      "classifier: Epoch:  1740 loss: 0.17903\n",
      "classifier: Epoch:  1745 loss: 0.17864\n",
      "classifier: Epoch:  1750 loss: 0.17908\n",
      "classifier: Epoch:  1755 loss: 0.17873\n",
      "classifier: Epoch:  1760 loss: 0.17904\n",
      "classifier: Epoch:  1765 loss: 0.17884\n",
      "classifier: Epoch:  1770 loss: 0.17899\n",
      "classifier: Epoch:  1775 loss: 0.17884\n",
      "classifier: Epoch:  1780 loss: 0.17863\n",
      "classifier: Epoch:  1785 loss: 0.17873\n",
      "classifier: Epoch:  1790 loss: 0.17868\n",
      "classifier: Epoch:  1795 loss: 0.17930\n",
      "classifier: Epoch:  1800 loss: 0.17858\n",
      "classifier: Epoch:  1805 loss: 0.17874\n",
      "classifier: Epoch:  1810 loss: 0.17877\n",
      "classifier: Epoch:  1815 loss: 0.17955\n",
      "classifier: Epoch:  1820 loss: 0.17877\n",
      "classifier: Epoch:  1825 loss: 0.17872\n",
      "classifier: Epoch:  1830 loss: 0.17887\n",
      "classifier: Epoch:  1835 loss: 0.17863\n",
      "classifier: Epoch:  1840 loss: 0.17912\n",
      "classifier: Epoch:  1845 loss: 0.17886\n",
      "classifier: Epoch:  1850 loss: 0.17857\n",
      "classifier: Epoch:  1855 loss: 0.17867\n",
      "classifier: Epoch:  1860 loss: 0.17875\n",
      "classifier: Epoch:  1865 loss: 0.17878\n",
      "classifier: Epoch:  1870 loss: 0.17857\n",
      "classifier: Epoch:  1875 loss: 0.17872\n",
      "classifier: Epoch:  1880 loss: 0.17851\n",
      "classifier: Epoch:  1885 loss: 0.17855\n",
      "classifier: Epoch:  1890 loss: 0.17874\n",
      "classifier: Epoch:  1895 loss: 0.17884\n",
      "classifier: Epoch:  1900 loss: 0.17850\n",
      "classifier: Epoch:  1905 loss: 0.17877\n",
      "classifier: Epoch:  1910 loss: 0.17854\n",
      "classifier: Epoch:  1915 loss: 0.17872\n",
      "classifier: Epoch:  1920 loss: 0.17857\n",
      "classifier: Epoch:  1925 loss: 0.17858\n",
      "classifier: Epoch:  1930 loss: 0.17851\n",
      "classifier: Epoch:  1935 loss: 0.17844\n",
      "classifier: Epoch:  1940 loss: 0.17850\n",
      "classifier: Epoch:  1945 loss: 0.17851\n",
      "classifier: Epoch:  1950 loss: 0.17849\n",
      "classifier: Epoch:  1955 loss: 0.17845\n",
      "classifier: Epoch:  1960 loss: 0.17854\n",
      "classifier: Epoch:  1965 loss: 0.17843\n",
      "classifier: Epoch:  1970 loss: 0.17883\n",
      "classifier: Epoch:  1975 loss: 0.17850\n",
      "classifier: Epoch:  1980 loss: 0.17845\n",
      "classifier: Epoch:  1985 loss: 0.17843\n",
      "classifier: Epoch:  1990 loss: 0.17872\n",
      "classifier: Epoch:  1995 loss: 0.17840\n",
      "classifier: Epoch:  2000 loss: 0.17871\n",
      "classifier: Epoch:  2005 loss: 0.17838\n",
      "classifier: Epoch:  2010 loss: 0.17853\n",
      "classifier: Epoch:  2015 loss: 0.17858\n",
      "classifier: Epoch:  2020 loss: 0.17842\n",
      "classifier: Epoch:  2025 loss: 0.17835\n",
      "classifier: Epoch:  2030 loss: 0.17833\n",
      "classifier: Epoch:  2035 loss: 0.17828\n",
      "classifier: Epoch:  2040 loss: 0.17839\n",
      "classifier: Epoch:  2045 loss: 0.17831\n",
      "classifier: Epoch:  2050 loss: 0.17856\n",
      "classifier: Epoch:  2055 loss: 0.17849\n",
      "classifier: Epoch:  2060 loss: 0.17844\n",
      "classifier: Epoch:  2065 loss: 0.17841\n",
      "classifier: Epoch:  2070 loss: 0.17807\n",
      "classifier: Epoch:  2075 loss: 0.17832\n",
      "classifier: Epoch:  2080 loss: 0.17807\n",
      "classifier: Epoch:  2085 loss: 0.17834\n",
      "classifier: Epoch:  2090 loss: 0.17815\n",
      "classifier: Epoch:  2095 loss: 0.17817\n",
      "classifier: Epoch:  2100 loss: 0.17842\n",
      "classifier: Epoch:  2105 loss: 0.17842\n",
      "classifier: Epoch:  2110 loss: 0.17907\n",
      "classifier: Epoch:  2115 loss: 0.17829\n",
      "classifier: Epoch:  2120 loss: 0.17845\n",
      "classifier: Epoch:  2125 loss: 0.17813\n",
      "classifier: Epoch:  2130 loss: 0.17830\n",
      "classifier: Epoch:  2135 loss: 0.17804\n",
      "classifier: Epoch:  2140 loss: 0.17829\n",
      "classifier: Epoch:  2145 loss: 0.17837\n",
      "classifier: Epoch:  2150 loss: 0.17817\n",
      "classifier: Epoch:  2155 loss: 0.17817\n",
      "classifier: Epoch:  2160 loss: 0.17826\n",
      "classifier: Epoch:  2165 loss: 0.17810\n",
      "classifier: Epoch:  2170 loss: 0.17826\n",
      "classifier: Epoch:  2175 loss: 0.17849\n",
      "classifier: Epoch:  2180 loss: 0.17832\n",
      "classifier: Epoch:  2185 loss: 0.17797\n",
      "classifier: Epoch:  2190 loss: 0.17812\n",
      "classifier: Epoch:  2195 loss: 0.17788\n",
      "classifier: Epoch:  2200 loss: 0.17773\n",
      "classifier: Epoch:  2205 loss: 0.17810\n",
      "classifier: Epoch:  2210 loss: 0.17791\n",
      "classifier: Epoch:  2215 loss: 0.17798\n",
      "classifier: Epoch:  2220 loss: 0.17767\n",
      "classifier: Epoch:  2225 loss: 0.17783\n",
      "classifier: Epoch:  2230 loss: 0.17765\n",
      "classifier: Epoch:  2235 loss: 0.17825\n",
      "classifier: Epoch:  2240 loss: 0.17804\n",
      "classifier: Epoch:  2245 loss: 0.17824\n",
      "classifier: Epoch:  2250 loss: 0.17840\n",
      "classifier: Epoch:  2255 loss: 0.17829\n",
      "classifier: Epoch:  2260 loss: 0.17815\n",
      "classifier: Epoch:  2265 loss: 0.17797\n",
      "classifier: Epoch:  2270 loss: 0.17814\n",
      "classifier: Epoch:  2275 loss: 0.17821\n",
      "classifier: Epoch:  2280 loss: 0.17797\n",
      "classifier: Epoch:  2285 loss: 0.17812\n",
      "classifier: Epoch:  2290 loss: 0.17816\n",
      "classifier: Epoch:  2295 loss: 0.17807\n",
      "classifier: Epoch:  2300 loss: 0.17809\n",
      "classifier: Epoch:  2305 loss: 0.17827\n",
      "classifier: Epoch:  2310 loss: 0.17797\n",
      "classifier: Epoch:  2315 loss: 0.17817\n",
      "classifier: Epoch:  2320 loss: 0.17814\n",
      "classifier: Epoch:  2325 loss: 0.17813\n",
      "classifier: Epoch:  2330 loss: 0.17848\n",
      "classifier: Epoch:  2335 loss: 0.17780\n",
      "classifier: Epoch:  2340 loss: 0.17794\n",
      "classifier: Epoch:  2345 loss: 0.17804\n",
      "classifier: Epoch:  2350 loss: 0.17822\n",
      "classifier: Epoch:  2355 loss: 0.17785\n",
      "classifier: Epoch:  2360 loss: 0.17801\n",
      "classifier: Epoch:  2365 loss: 0.17782\n",
      "classifier: Epoch:  2370 loss: 0.17805\n",
      "classifier: Epoch:  2375 loss: 0.17772\n",
      "classifier: Epoch:  2380 loss: 0.17796\n",
      "classifier: Epoch:  2385 loss: 0.17808\n",
      "classifier: Epoch:  2390 loss: 0.17808\n",
      "classifier: Epoch:  2395 loss: 0.17811\n",
      "classifier: Epoch:  2400 loss: 0.17804\n",
      "classifier: Epoch:  2405 loss: 0.17797\n",
      "classifier: Epoch:  2410 loss: 0.17793\n",
      "classifier: Epoch:  2415 loss: 0.17769\n",
      "classifier: Epoch:  2420 loss: 0.17790\n",
      "classifier: Epoch:  2425 loss: 0.17797\n",
      "classifier: Epoch:  2430 loss: 0.17794\n",
      "classifier: Epoch:  2435 loss: 0.17785\n",
      "classifier: Epoch:  2440 loss: 0.17794\n",
      "classifier: Epoch:  2445 loss: 0.17783\n",
      "classifier: Epoch:  2450 loss: 0.17807\n",
      "classifier: Epoch:  2455 loss: 0.17810\n",
      "classifier: Epoch:  2460 loss: 0.17791\n",
      "classifier: Epoch:  2465 loss: 0.17783\n",
      "classifier: Epoch:  2470 loss: 0.17808\n",
      "classifier: Epoch:  2475 loss: 0.17776\n",
      "classifier: Epoch:  2480 loss: 0.17800\n",
      "classifier: Epoch:  2485 loss: 0.17794\n",
      "classifier: Epoch:  2490 loss: 0.17782\n",
      "classifier: Epoch:  2495 loss: 0.17794\n",
      "classifier: Epoch:  2500 loss: 0.17780\n",
      "classifier: Epoch:  2505 loss: 0.17786\n",
      "classifier: Epoch:  2510 loss: 0.17796\n",
      "classifier: Epoch:  2515 loss: 0.17772\n",
      "classifier: Epoch:  2520 loss: 0.17770\n",
      "classifier: Epoch:  2525 loss: 0.17771\n",
      "classifier: Epoch:  2530 loss: 0.17811\n",
      "classifier: Epoch:  2535 loss: 0.17787\n",
      "classifier: Epoch:  2540 loss: 0.17773\n",
      "classifier: Epoch:  2545 loss: 0.17747\n",
      "classifier: Epoch:  2550 loss: 0.17754\n",
      "classifier: Epoch:  2555 loss: 0.17788\n",
      "classifier: Epoch:  2560 loss: 0.17776\n",
      "classifier: Epoch:  2565 loss: 0.17787\n",
      "classifier: Epoch:  2570 loss: 0.17774\n",
      "classifier: Epoch:  2575 loss: 0.17795\n",
      "classifier: Epoch:  2580 loss: 0.17781\n",
      "classifier: Epoch:  2585 loss: 0.17781\n",
      "classifier: Epoch:  2590 loss: 0.17783\n",
      "classifier: Epoch:  2595 loss: 0.17773\n",
      "classifier: Epoch:  2600 loss: 0.17773\n",
      "classifier: Epoch:  2605 loss: 0.17786\n",
      "classifier: Epoch:  2610 loss: 0.17773\n",
      "classifier: Epoch:  2615 loss: 0.17764\n",
      "classifier: Epoch:  2620 loss: 0.17776\n",
      "classifier: Epoch:  2625 loss: 0.17766\n",
      "classifier: Epoch:  2630 loss: 0.17771\n",
      "classifier: Epoch:  2635 loss: 0.17746\n",
      "classifier: Epoch:  2640 loss: 0.17769\n",
      "classifier: Epoch:  2645 loss: 0.17772\n",
      "classifier: Epoch:  2650 loss: 0.17756\n",
      "classifier: Epoch:  2655 loss: 0.17767\n",
      "classifier: Epoch:  2660 loss: 0.17757\n",
      "classifier: Epoch:  2665 loss: 0.17769\n",
      "classifier: Epoch:  2670 loss: 0.17753\n",
      "classifier: Epoch:  2675 loss: 0.17747\n",
      "classifier: Epoch:  2680 loss: 0.17752\n",
      "classifier: Epoch:  2685 loss: 0.17743\n",
      "classifier: Epoch:  2690 loss: 0.17732\n",
      "classifier: Epoch:  2695 loss: 0.17749\n",
      "classifier: Epoch:  2700 loss: 0.17761\n",
      "classifier: Epoch:  2705 loss: 0.17763\n",
      "classifier: Epoch:  2710 loss: 0.17754\n",
      "classifier: Epoch:  2715 loss: 0.17756\n",
      "classifier: Epoch:  2720 loss: 0.17772\n",
      "classifier: Epoch:  2725 loss: 0.17758\n",
      "classifier: Epoch:  2730 loss: 0.17751\n",
      "classifier: Epoch:  2735 loss: 0.17765\n",
      "classifier: Epoch:  2740 loss: 0.17759\n",
      "classifier: Epoch:  2745 loss: 0.17752\n",
      "classifier: Epoch:  2750 loss: 0.17766\n",
      "classifier: Epoch:  2755 loss: 0.17753\n",
      "classifier: Epoch:  2760 loss: 0.17748\n",
      "classifier: Epoch:  2765 loss: 0.17747\n",
      "classifier: Epoch:  2770 loss: 0.17755\n",
      "classifier: Epoch:  2775 loss: 0.17745\n",
      "classifier: Epoch:  2780 loss: 0.17733\n",
      "classifier: Epoch:  2785 loss: 0.17741\n",
      "classifier: Epoch:  2790 loss: 0.17759\n",
      "classifier: Epoch:  2795 loss: 0.17772\n",
      "classifier: Epoch:  2800 loss: 0.17754\n",
      "classifier: Epoch:  2805 loss: 0.17765\n",
      "classifier: Epoch:  2810 loss: 0.17722\n",
      "classifier: Epoch:  2815 loss: 0.17748\n",
      "classifier: Epoch:  2820 loss: 0.17714\n",
      "classifier: Epoch:  2825 loss: 0.17733\n",
      "classifier: Epoch:  2830 loss: 0.17743\n",
      "classifier: Epoch:  2835 loss: 0.17749\n",
      "classifier: Epoch:  2840 loss: 0.17735\n",
      "classifier: Epoch:  2845 loss: 0.17736\n",
      "classifier: Epoch:  2850 loss: 0.17728\n",
      "classifier: Epoch:  2855 loss: 0.17748\n",
      "classifier: Epoch:  2860 loss: 0.17729\n",
      "classifier: Epoch:  2865 loss: 0.17752\n",
      "classifier: Epoch:  2870 loss: 0.17729\n",
      "classifier: Epoch:  2875 loss: 0.17730\n",
      "classifier: Epoch:  2880 loss: 0.17927\n",
      "classifier: Epoch:  2885 loss: 0.17747\n",
      "classifier: Epoch:  2890 loss: 0.17741\n",
      "classifier: Epoch:  2895 loss: 0.17746\n",
      "classifier: Epoch:  2900 loss: 0.17752\n",
      "classifier: Epoch:  2905 loss: 0.17755\n",
      "classifier: Epoch:  2910 loss: 0.17752\n",
      "classifier: Epoch:  2915 loss: 0.17748\n",
      "classifier: Epoch:  2920 loss: 0.17750\n",
      "classifier: Epoch:  2925 loss: 0.17726\n",
      "classifier: Epoch:  2930 loss: 0.17724\n",
      "classifier: Epoch:  2935 loss: 0.17733\n",
      "classifier: Epoch:  2940 loss: 0.17766\n",
      "classifier: Epoch:  2945 loss: 0.17744\n",
      "classifier: Epoch:  2950 loss: 0.17734\n",
      "classifier: Epoch:  2955 loss: 0.17722\n",
      "classifier: Epoch:  2960 loss: 0.17722\n",
      "classifier: Epoch:  2965 loss: 0.17722\n",
      "classifier: Epoch:  2970 loss: 0.17717\n",
      "classifier: Epoch:  2975 loss: 0.17737\n",
      "classifier: Epoch:  2980 loss: 0.17757\n",
      "classifier: Epoch:  2985 loss: 0.17726\n",
      "classifier: Epoch:  2990 loss: 0.17734\n",
      "classifier: Epoch:  2995 loss: 0.17732\n"
     ]
    }
   ],
   "source": [
    "# train all the classifiers...\n",
    "epochs = 3000\n",
    "epoch_display_rate = 5\n",
    "clf.train()\n",
    "for epoch in range(epochs):\n",
    "  running_loss = 0.0\n",
    "  for i, data in enumerate(train_loader, 0):\n",
    "    inputs, labels = data\n",
    "    # zero grad to remove previous gradient values\n",
    "    clfLinearOptimizer.zero_grad()\n",
    "    # forward propagation\n",
    "    outputs = clf(inputs)\n",
    "    loss = clfLinearCriterion(outputs, labels)\n",
    "    # backward propagation\n",
    "    loss.backward()\n",
    "    # optimize\n",
    "    clfLinearOptimizer.step()\n",
    "    running_loss += loss.item()\n",
    "  # display statistics\n",
    "  if (epoch % epoch_display_rate == 0):\n",
    "    print(f'classifier: Epoch: {epoch:5d} loss: {running_loss / 2000:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6fd23952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses for classifier\n",
      "\n",
      "\n",
      "Accuracy on test points: 95%\n"
     ]
    }
   ],
   "source": [
    "print(\"Losses for classifier\")\n",
    "\n",
    "correct, total = 0, 0\n",
    "# no.grad context to not have to calculate the gradiants\n",
    "with torch.no_grad():\n",
    "    for ___, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        outputs = clf(inputs)\n",
    "        ____, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        # update results\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    print(f'\\n\\nAccuracy on test points: {100 * correct // total}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "df339f56a81c1fdf7aa70064d8accff0b478570d3e962af223420cf1240a28b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
